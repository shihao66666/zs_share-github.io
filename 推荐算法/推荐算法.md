# 推荐系统

**为什么需要推荐系统呢？**

在大数据时代，随着app的增多，市场竞争越来越激烈，同质化产品多。每个用户使用产品都有自己的喜好、目的，如果app对每个用户呈现的内容都是相同的，用户很难找到自己感兴趣的东西，慢慢的就会流失掉了；并且在信息爆炸的现在，信息量太多，用户根本看不过来，我们要根据用户的喜好将我们的内容推荐给他，留住用户。

推荐系统有两个主题：用户，内容（商品）

给什么用户推荐什么内容？

1. 基于内容的相似性：基于内容做相似性，用户看过这个内容，停留的时间长，点赞了等行为，我就认为用户喜欢这种类型的内容。平台首先提取内容的特征（tf-idf），找到用户喜欢的内容，这个内容有什么特征？根据两个内容特征的相似性推荐给用户内容。
   - 这个方法的优点是每个用户都是独立的个体，我根据内容特征找相似性；解释型强，用户喜欢看这个类型的内容，就给他推荐有这个特征的内容；新的内容也可以被直接推荐，这个新的内容只要有特征，就会被推荐。
   - 这个方法也是有缺点的：内容的特征比较难提取，如果是视频内容呢？只能对用户之前喜欢的内容，无法挖掘用户对新内容的潜在需求；对于新用户，没有历史喜好，无法进行推荐
2. 协同过滤算法：基于用户的协同过滤----根据用户的相似性，其他相似的用户对内容有哪些喜好，就推荐这种类型的内容；基于物品的协同过滤----基于内容的相似性，例如大量用户看过A类型和B、C类的内容，这时候恰好有个用户看过A和B，那我就给他推荐C，他大概率会喜欢

## 1. 关联规则

### 1. 什么是关联规则

关联规则是指事物之间的相互联系，反映了一个事物与其他事物的关联性。如果两个或者多个事物存在一定的关联关系，那么其中的一个事物是可以通过其他的事物进行预测的。

举个例子：以啤酒与尿不湿的例子来讲解----在一家超市里，人们发现了一个非常奇怪的现象，啤酒和尿不湿两个没有关系的商品摆放在了一起，但是这个摆放使得尿不湿和啤酒的销量大幅度增加了。为什么会出现这样的现象呢？是因为买尿不湿的通常是父亲，他们在买尿不湿的时候如果看到了啤酒，将有很大的概率会购买。

通过购买的数据发现，父亲买完尿布之后，有很大的概率（频率代替概率）去买啤酒，那么这就是简单的两个商品之间的内在关联。

**音乐、视频、图书的推荐。**

基于历史的数据，发现大规模的用户的某些行为存在先后顺序：

比如通过大规模的数据发现，大量的用户看剧列表：小时达->上海堡垒。如果一个新用户在看完小时代之后直接给他推荐上海堡垒，这个新用户大概率会看的。

通过以上案例，有点了解了关联规则，它是挖掘两个或者多个变量之间有没有关系，一个事物出现了，另一个事物出现的概率大不大。

### 2. 知识介绍

关联规则通常使用support、confidence、lift三个指标，其余指标不常使用。

- 事物：一个样本称为一个事物。就像购买商品后去付费，有一个发票。这个发票会显示购买了哪些东西。购买了牛奶，果冻，面包，啤酒。这是一个订单，也称为一个事物
  - 每个事物由多个属性确定，这个属性就称为项集
  - X==>Y含义：
    - X和Y都是项集
    - X被称为规则前项
    - Y被称为规则后项
    - 规则前项出现了，规则后项出现的概率大不大，如果这个概率比较大，并且两个属性不是独立的，那么我就有理由判断两个事物是由联系的。
  - 事物只包含到了涉及的项目，不包含项目的具体信息
    - 举例：就比如超市的关联规则，事物是顾客购买了哪些商品，不包含商品的价格和数量。
- 支持度（support）：
  - 一个项集或者一个规则在所有事物中出现的频率
  - 规则X==>Y表示物品集X对物品集Y的支持度，也就是物品集X和物品集Y同时出现的概率
- 置信度（confidence）：
  - 确定Y在包含X的事物中出现的频繁程度
  - P(Y|X) = P(XY) / P(X) 就是在规则前项出现的情况下，规则后项发生的概率
- 阈值影响
  - 模型开始之前需要确定最小的支持度和最小的置信度，若关联规则X->Y的支持度和置信度分别大于或等于指定的阈值，则称为强关联性，否则就是弱关联性
- 提升度（lift）
  - 物品集A的出现对B的出现提升了多少
  - 如果A和B两个变量之间是相互独立的，即使在选取的实验样本中，两个变量同时出现的频率比较高，但是在另一份样本中，A没有出现,B出现的频率还是比较高。没有考虑两个变量之间是独立的。
  - lift(A==>B) = confidence(A==>B) / support(B) = P(B|A)/P(B)
  - 如果lift = 1表示A和B是相互独立的，如果大于1表示存在一定的关联

### 3. 算法使用

- 导包

  - ```python
    import pandas as pd
    from mlxtend.frequent_patterns import apriori
    from mlxtend.frequent_patterns import association_rules
    ```

- 数据集构建

  - 使用算法前首先需要把数据集构建成ONE_HOT类型的编码。即0表示属性出现，1表示属性没有出现。不考虑数量、单位。只考虑出现or没有出现

- 选择频繁项集

  - ```python
    frequent_itemsets = apriori(df, min_support=0.50, use_colnames=True)
    # 选择最小支持度
    # 数据集越大，最小阈值也应该设置的比较小
    ```

- 计算规则

  - ```python
    rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)
    # 选择最小的lift指标
    ```

  - 根据具体的业务现状选择合适的最小阈值


## 2. 协同过滤

协同过滤在推荐算法中比较常见，通过分析用户之间的相似性或者事物之间的相似性来预测用户可能感兴趣的内容，并将此内容推荐给用户。

这个相似性可以是用户特征的相似性、也可以是用户喜好、用户行为的相似性。

协同过滤大概分为3类：

​	基于用户的协同过滤（userCF）

​	基于物品的协同过滤（itemCF）

​	基于模型的协同过滤（modelCF）

### 1. 基于用户的协同过滤

UserCF是找到用户与用户之间的相似性，基于兴趣相似的用户喜欢的产品推荐给目标用户。举个例子：在数据中表现为A和B用户有购买相同商品，并且他们给商品的评分都一样，我可能就有理由相信他们是相似的用户了。用户A有一些商品是用户B没有购买的，那我就把A买过的商品推荐给B，B大概率就会购买。

**算法实现步骤**

1. 找到与目标用户相似的用户群体

   - 用户与用户之间怎么计算相似度？

     - 以现实举例子，我和我的朋友都喜欢玩王者荣耀，经常在一起五排，那我们在就会有很多的话题，因为我们的兴趣爱好都一样，可以根据这一点计算用户之间的相似性

     - 对于用户u和用户v，令N(u)代表用户u喜欢的物品合集，令N(v)代表用户v喜欢的物品合集。N(u)∩N(v)代表的是用户u和用户v都喜欢的物品，N(u)∪N(v)代表的是用户u和用户v喜欢的物品的合集，那么可以利用以下公式来简单计算相似度:![math](assets/math.svg)

       - 上述公式叫做Jaccard公式，直观上理解就是，将用户u与用户v都喜欢的物品的数量除以他们喜欢物品的总和，如果u和v喜欢的物品是一模一样的，则u和v的相似度为1。

     - 另外一种计算相似度的公式：余弦相似度![math (1)](assets/math (1).svg)

       - 上述公式的分母部分代表的是u喜欢的物品的数量与v喜欢的物品的数量的乘积，而不再是他们之间的交集。

       - | u    | {王者荣耀，金铲铲，和平精英}           |
         | ---- | -------------------------------------- |
         | v    | {王者荣耀，金铲铲，蛋仔派对，元梦之星} |

         $$
         Wμv = \frac{|{王者荣耀，金铲铲，和平精英} ∩ {王者荣耀，金铲铲，蛋仔派对，元梦之星}|} { \sqrt{|{王者荣耀，金铲铲，和平精英} || {王者荣耀，金铲铲，蛋仔派对，元梦之星}}} = \frac{2}{\sqrt{12}}
         $$

       - 这样我们就可以计算出两两用户之间的相似度了。我们通常会维护一个用户相似度表

2. 找到相似的用户群体最喜欢的产品，并且目标用户还没有接触过的产品。

   - 我通过上面的余弦相似度计算出来用户的相似度表，我找了相似度最高的K个用户，但是这K个用户喜欢的商品有很多，我怎么给目标用户取推荐呢？哪个商品优先推荐呢？
     - 通过计算用户对每个商品的感兴趣程度，如果遍历除去目标用户喜欢的物品之外的K歌用户喜欢的商品，计算每个商品有多少用户喜欢，和目标用户相似度如何？![math (2)](assets/math (2).svg)
       - 其中S(μ，K)表示与用户μ相似的K歌用户，N（i）表示对物品i喜欢的用户集合，Wμv就是用户μ和用户v之间的相似度，Rvi表示用户v对物品i的兴趣，因为选取的物品都是v喜欢的，所以Rvi=1
       - 计算出相似用户对物品i的喜欢程度，逆序排序，选取一定数量的商品，进行推荐。

   [基于用户的协同过滤算法（UserCF）原理以及代码实践 - 简书 (jianshu.com)](https://www.jianshu.com/p/7c5d9c008be9)
   
   
   
   

#### 1.1 算法流程

以下是UserCF的算法流程。

首先，我们需要一个user-time表，用户喜欢的物品表，它记录了每个用户喜欢的商品，用户喜好

| 用户 | 喜爱的物品 |
| ---- | ---------- |
| A    | {a,b,d}    |
| B    | {a,c,d}    |
| C    | {b,e}      |
| D    | {c,d,e}    |

有了用户喜好列表之后，我们需要计算出来用户与用户之间之间的相似度，即用户相似度矩阵。我们对两两用户计算出来他们的相似度，时间复杂度是O（n**2），当用户量比较大的时候，非常耗时。但是如果用户之间没有共同的喜好时，我们就没必要计算了。

也就是说我们没有必要计算两两用户之间的相似性，我们只需要计算有共同爱好的用户之间的相似度。所以，我们建立商品-用户表，看用户之间有没有共同的爱好。

| 物品 | 喜爱它的用户 |
| ---- | ------------ |
| a    | {A,B}        |
| b    | {A,C}        |
| c    | {B,D}        |
| d    | {A,B,D}      |
| e    | {C,D}        |

有了这张表之后，我们就将零散的用户通过物品关联起来，划分成为了几个小圈子。

我们这么做的目的是为了快速找到有共同爱好的用户，计算他们爱好之间的交集  $|N(μ)∩N(v)|$，快速计算用户喜爱物品交集矩阵W：

| A    | B    | C    | D    |      |
| ---- | ---- | ---- | ---- | ---- |
| A    | 0    | 2    | 1    | 1    |
| B    | 2    | 0    | 0    | 2    |
| C    | 1    | 0    | 0    | 1    |
| D    | 1    | 2    | 1    | 0    |

有了用户交集计算出来的矩阵，接下来我们需要每个用喜爱的商品件数。之前的user-item矩阵就可以计算出来，下表就是根据余弦相似度计算出来的用户相似度

|      | A    | B    | C    | D    |
| ---- | ---- | ---- | ---- | ---- |
| A    | 0    | 0.67 | 0.41 | 0.33 |
| B    | 0.67 | 0    | 0    | 0.67 |
| C    | 0.41 | 0    | 0    | 0.41 |
| D    | 0.33 | 0.67 | 0.41 | 0    |

当用户相似度矩阵建立好之后，我们就可以进行推荐了。

比如：我要对C用户进行推荐，A和D用户与C用户爱好相似。A用户喜爱的商品列表是{a,b,d}，D用户喜爱的商品列表是{c,d,e}，C用户喜爱的商品列表是{b,e}，A,D喜爱的物品交集是{a,b,c,d,e}，为了避免推荐重复的产品，排除掉C用户已经喜欢的商品，剩下{a,c,d}商品，计算哪件商品用户C更感兴趣。

P(C,a) = W'[C]'[A] = 0.41

P(C,c) = W[C]'[D] = 0.41

P(C,d) = W[C]'[A] + W[C]'[D] = 0.82

按照用户对各个商品的感兴趣程度逆序排列，推荐前K个商品

#### 1.2 相似度算法的改进

上面的预先相似度的计算规则比较简单，只计算了用户喜爱商品的交集和各个用户喜爱的商品数目。对于热销商品来说，大部门用户都会购买，但是我不能说明两个用户就是相似的。对于销量正常或者销量比较少的产品，如果两个用户都够买了，那么就说明两个用户之间的兴趣爱好是有一定的相似度的

因此John.S.Breese提出了以下相似度计算公式：![math](assets/math-17055856446603.svg)

相比于之前的余弦相似度，这个算法加入了一个惩罚项，降低了对热门商品的影响



### 2. 基于物品的协同过滤

```

```





### 3. 矩阵分解算法

协同过滤算法的可解释性非常强，基于用户的协同过滤算法是基于用户的历史购买行为，判断哪些用户对他们购买的物品有相似的评价，就认为他们的兴趣相似，那兴趣相似的用户喜欢什么，目标用户也有很大的概率喜欢这个件商品。

基于物品的协同过滤是计算两个物品之间有多少用户一起购买了，购买了A的情况下也购买了B，那就认为这A和B是相似的物品。

但是协同过滤算法对于热门商品的推荐效果很明显，因为热门商品大家都会购买，和其他商品相似度也比较高；但是对于长尾理论哪些小众的商品，他们的矩阵是非常稀疏的，甚少会和其他物品产生相似性，因此推荐的次数就会少。

为了解决这些问题，同时增加模型的泛华能力，就出现了矩阵分解技术，在协同过滤贡献矩阵的基础上，使用更稠密的隐向量表示用户和物品，挖掘用户和物品之间隐含的特征，在一定程度上弥补了协同过滤算法处理稀疏矩阵的缺点。

除此之外，协同过滤算法仅用到了用户和物品这两个信息，没有深入挖掘用户和物品的特征，例如：用户年龄、性别、消费习惯，物品特征，商品分类，以及时间趋势。在一定程度上造成了有效信息的遗漏。

#### 矩阵分解算法的原理

![image-20240203154723836](assets/image-20240203154723836.png)

图示是协同过滤算法和矩阵分解的简单示意图

协同过滤算法是基于目标用户的历史观看记录，找到跟目标用户joe看过同样视频的相似用户，根据相似用户看过的视频推荐给目标用户。

矩阵分解是为每一个用户和视频生成一个隐向量，将用户和视频定位到隐向量表示的空间中，距离相近的用户和视频他们的兴趣点相似。在推荐的过程中，会计算每个视频与用户的距离，找到距离最近的K个视频进行推荐。

但是问题来了，如何分解矩阵呢？生成每一个用户和物品的隐向量呢？并且分解之后，相似的用户和用户感兴趣的物品距离最近呢？

在矩阵分解算法的框架下，用户和物品的隐向量就是分解协同过滤算法生成的共现矩阵得到的。

![image-20240203160212113](assets/image-20240203160212113.png)

矩阵分解算法就是将m×n的共现矩阵分解成，m×k，k×n的两个矩阵

m表示用户数量，n表示物品数量，k表示隐向量的维度。

K的大小表达了模型表达能力的强弱，模型的复杂程度。k值越小，表达能力越弱，泛化能力越强。k值越大，表达能力越强，模型泛华能力越弱。

#### 矩阵分解

矩阵分解有三种方法：特征值分解，奇异值分解（svd）、奇异值分解

- 特征值分解：

  - 特征值分解起码要是方阵，但是通常情况下物品和用户数量是远远的不相等的，所以，不能用特征值进行分解。

- 奇异值分解：

  - 奇异值分解的具体流程：将矩阵M（m×n）分解成 M = U∑V，U是m×m的矩阵，V是n×n的矩阵，∑是m×n的对角矩阵。取对角阵∑中较大的K个元素作为隐特征，删除∑其他维度和U,V的对应的维度。完美的解决了矩阵分解的问题。但是奇异值分解有使用条件以及缺点，导致他不使用大量的数据
  - 缺点1：奇异值矩阵要求待分解的共现矩阵是稠密的。在互联网场景下，用户的历史行为很少，用户物品的共现矩阵比较稀疏，不使用这个奇异值分解。
  - 缺点2：传统奇异值的分解复杂度达到了$O(mn^2)$,这对于互联网企业存在的大数据情况下不使用如此复杂的模型。

- 梯度下降法：

  梯度下降法的目的是想要真实值和预测的值的误差不断的变小，达到极小值。目标函数：$(真实值 - 预测值)^2$最小

![image-20240203172030678](assets/image-20240203172030678.png)

​	梯度下降法的目标函数，目的是让用户的原始评分rui和用户向量与物品向量之积的差最小。

K值用户评分样本的集合，为了防止模型过拟合，加入了正则化项。

![image-20240203172240247](assets/image-20240203172240247.png)

加入正则化的目的是为了让目标函数的变动更稳定，不至于变化幅度特别大。其中λ是正则化项系数，平方表示是L2正则化，1次方表示是L1正则化。

**消除用户和物品的打分偏差**

从用户角度来看，不同的用户对打分有不一样的理解，比如在满分为5分的情况下，有的用户认为3分就是比较高的分数了，但是有的用户则认为3分是低分，所以这样的认知也会导致不同用户的评分差异。

从物品角度来看，不同类别的产品的打分也存在一些差异，电子产品的平均分和日用品的平均分也存在一些差异。

为了消除用户和物品的打分的差异，常用的做法就是在矩阵分解的时候加入用户和物品的偏差向量

![image-20240203173933400](assets/image-20240203173933400.png)

μ表示全局偏差，bi表示物品偏差系数，可使用物品i的平均评分，bu表示用户的偏差系数，可使用用户u对所有商品的平均分。

加入了用户和物品的隐向量之后，就更能表示不同的用户对不同的物品的“真实”态度差异，也更容易捕捉有价值的信息。

**优点和缺点**

- 优点
  - 模型复杂程度低
  - 泛华能力强，在一定程度上解决了稀疏矩阵的问题
  - 可拓展性强
- 缺点：和协同过滤一样，还是无法加入用户和物品的特征和上下文信息，在缺乏用户历史行为信息的情况下，无法进行有效的推荐。
